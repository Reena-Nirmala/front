{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59620f2e-5a1f-400e-b6e7-868655fe9666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from pyttsx3) (302)\n",
      "Collecting pypiwin32\n",
      "  Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Requirement already satisfied: comtypes in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from pyttsx3) (1.1.10)\n",
      "Installing collected packages: pypiwin32, pyttsx3\n",
      "Successfully installed pypiwin32-223 pyttsx3-2.90\n"
     ]
    }
   ],
   "source": [
    "pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71195301-d57d-4a03-99be-23a14d97fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.6/232.6 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from PyPDF2) (4.3.0)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ff4963-0b0f-4456-bcc8-90400131372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting language_tool_python\n",
      "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from language_tool_python) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from language_tool_python) (4.64.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from requests->language_tool_python) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from requests->language_tool_python) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from requests->language_tool_python) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from requests->language_tool_python) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from tqdm->language_tool_python) (0.4.5)\n",
      "Installing collected packages: language_tool_python\n",
      "Successfully installed language_tool_python-2.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install language_tool_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "199c6c31-caa7-4928-a4b6-8ec1bb5afd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "     ---------------------------------------- 7.9/7.9 MB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 11.7 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "     -------------------------------------- 311.7/311.7 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp39-none-win_amd64.whl (277 kB)\n",
      "     ------------------------------------ 277.2/277.2 kB 682.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.3.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "     -------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "Successfully installed fsspec-2023.10.0 huggingface-hub-0.19.4 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6386ad00-1682-429c-b8b0-c10ff795ee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Downloading pymongo-4.6.0-cp39-cp39-win_amd64.whl (472 kB)\n",
      "     -------------------------------------- 472.7/472.7 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting dnspython<3.0.0,>=1.16.0\n",
      "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "     -------------------------------------- 300.4/300.4 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.4.2 pymongo-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97190987-077a-4807-9c9a-a3987c7ceb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of points:  4\n",
      "Enter x-coordinate for point 1:  -11\n",
      "Enter y-coordinate for point 1:  3\n",
      "Enter x-coordinate for point 2:  8\n",
      "Enter y-coordinate for point 2:  5\n",
      "Enter x-coordinate for point 3:  -3\n",
      "Enter y-coordinate for point 3:  2\n",
      "Enter x-coordinate for point 4:  9\n",
      "Enter y-coordinate for point 4:  17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distances: [19, 11, 19]\n",
      "Mode of Distances: 19\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return math.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "\n",
    "def calculate_distances(coordinates):\n",
    "    distances = []\n",
    "    for i in range(len(coordinates) - 1):\n",
    "        distance = euclidean_distance(coordinates[i], coordinates[i + 1])\n",
    "        distances.append(int(distance))  # Taking the floor integer value of the result\n",
    "    return distances\n",
    "\n",
    "def find_mode(distance_list):\n",
    "    counter = Counter(distance_list)\n",
    "    modes = counter.most_common()\n",
    "    return modes[0][0]\n",
    "\n",
    "# Get input from the user\n",
    "n = int(input(\"enter number of cooordinates\"))\n",
    "num_points = int(input(\"Enter the number of points: \"))\n",
    "point_coordinates = []\n",
    "for i in range(num_points):\n",
    "    x = float(input(f\"Enter x-coordinate for point {i + 1}: \"))\n",
    "    y = float(input(f\"Enter y-coordinate for point {i + 1}: \"))\n",
    "    point_coordinates.append((x, y))\n",
    "\n",
    "# Calculate distances and find mode\n",
    "distances = calculate_distances(point_coordinates)\n",
    "mode_distance = find_mode(distances)\n",
    "\n",
    "# Display results\n",
    "print(\"Euclidean Distances:\", distances)\n",
    "print(\"Mode of Distances:\", mode_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f1eed4-0330-4fd0-8d19-611630a2efbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting virtualenvwrapper-win\n",
      "  Downloading virtualenvwrapper_win-1.2.7-py3-none-any.whl (18 kB)\n",
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.24.7-py3-none-any.whl (3.8 MB)\n",
      "     ---------------------------------------- 3.8/3.8 MB 11.5 MB/s eta 0:00:00\n",
      "Collecting distlib<1,>=0.3.7\n",
      "  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
      "     ------------------------------------- 468.9/468.9 kB 10.0 MB/s eta 0:00:00\n",
      "Collecting platformdirs<5,>=3.9.1\n",
      "  Downloading platformdirs-4.0.0-py3-none-any.whl (17 kB)\n",
      "Collecting filelock<4,>=3.12.2\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: distlib, platformdirs, filelock, virtualenv, virtualenvwrapper-win\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 2.5.2\n",
      "    Uninstalling platformdirs-2.5.2:\n",
      "      Successfully uninstalled platformdirs-2.5.2\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.6.0\n",
      "    Uninstalling filelock-3.6.0:\n",
      "      Successfully uninstalled filelock-3.6.0\n",
      "Successfully installed distlib-0.3.7 filelock-3.13.1 platformdirs-4.0.0 virtualenv-20.24.7 virtualenvwrapper-win-1.2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install virtualenvwrapper-win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9456549d-a773-47d7-956c-64016c7a50a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting django\n",
      "  Downloading Django-4.2.7-py3-none-any.whl (8.0 MB)\n",
      "     ---------------------------------------- 8.0/8.0 MB 15.0 MB/s eta 0:00:00\n",
      "Collecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "     -------------------------------------- 41.2/41.2 kB 658.4 kB/s eta 0:00:00\n",
      "Collecting asgiref<4,>=3.6.0\n",
      "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Collecting tzdata\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from asgiref<4,>=3.6.0->django) (4.3.0)\n",
      "Installing collected packages: tzdata, sqlparse, asgiref, django\n",
      "Successfully installed asgiref-3.7.2 django-4.2.7 sqlparse-0.4.4 tzdata-2023.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9344e3a-9fba-4e4c-b23c-7cc690e015c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd1a80-2431-44c3-92d9-b103523e6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8690e58-126c-4150-a51c-eab01af2129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SentencePiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     -------------------------------------- 977.6/977.6 kB 5.6 MB/s eta 0:00:00\n",
      "Installing collected packages: SentencePiece\n",
      "Successfully installed SentencePiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877c1341-d9c4-47da-ad8c-750ead1b5b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.3.6-py3-none-any.whl (220 kB)\n",
      "     -------------------------------------- 220.9/220.9 kB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 75.0/75.0 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting typing-extensions<5,>=4.5\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "     -------------------------------------- 381.9/381.9 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from anyio<4,>=3.5.0->openai) (3.3)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 76.9/76.9 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.9.14)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting pydantic-core==2.14.5\n",
      "  Downloading pydantic_core-2.14.5-cp39-none-win_amd64.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 6.3 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.5)\n",
      "Installing collected packages: typing-extensions, h11, distro, annotated-types, pydantic-core, httpcore, pydantic, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "Successfully installed annotated-types-0.6.0 distro-1.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.6 pydantic-2.5.2 pydantic-core-2.14.5 typing-extensions-4.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c61623-716d-4667-94cf-c36ce745b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "myclient = pymongo.MongoClient(\"mongodb+srv://Summarease:Svnrr%402002@cluster1.rq4cic8.mongodb.net/\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"customers\"]\n",
    "mydict = { \"name\": \"John\", \"address\": \"Highway 37\" }\n",
    "x = mycol.insert_one(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4e733d-baa3-4481-8bd7-7b1acafef343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1. Want to login? \n",
      " 2. Want to register? 1\n",
      "UserName :  Reena\n",
      "Password :  Svnrr@2002\n",
      "1. Want to login? \n",
      " 2. Want to register? 2\n",
      "UserName :  Reena\n",
      "email :  sreenasvnrr@gmail.com\n",
      "password :  Svhcg@423\n",
      "re-enter password :  Svhcg@423\n",
      "1. Want to login? \n",
      " 2. Want to register? 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while(True):\n",
    "    user = input(\"1. Want to login? \\n 2. Want to register?\")\n",
    "    if user=='0':\n",
    "        break\n",
    "    else:\n",
    "        if user=='1':     # LOGIN\n",
    "            user_name = input(\"UserName : \")\n",
    "            password = input(\"Password : \")\n",
    "        elif user == '2':\n",
    "            user_name = input(\"UserName : \")\n",
    "            email = input(\"email : \")\n",
    "            password = input(\"password : \")\n",
    "            re_enter_password = input(\"re-enter password : \")\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b203ad-265e-4750-a1f8-b1435b8214b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.1-cp39-cp39-win_amd64.whl (192.2 MB)\n",
      "     -------------------------------------- 192.2/192.2 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\reena_savireddy\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414ae922-7943-498f-86c5-ad319b5a07ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26832e0f5363433ea8f1c9ff390ce842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reena_savireddy\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\reena_savireddy\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc260331b2ba4d73a65101b4c9952ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3731822cd80479ab6168e29a2b75ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd6c16e032440f0946adecc7348003b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e69496a1964c4884edb10d4001d256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14140\\444292481.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Tokenize input texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtokenized_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Create DataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14140\\444292481.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Tokenize input texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtokenized_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Create DataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2796\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2797\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2798\u001b[1;33m             \u001b[0mencodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2799\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2800\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2855\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2856\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   2857\u001b[0m                 \u001b[1;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2858\u001b[0m                 \u001b[1;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Assuming you have a list of sentences (texts) and corresponding labels (0 or 1)\n",
    "# For simplicity, I'm using the IMDb dataset as an example\n",
    "# Replace this with your own dataset and preprocessing steps\n",
    "texts = [\"This is a positive review.\", \"This is a negative review.\", ...]\n",
    "labels = [1, 0, ...]\n",
    "\n",
    "# Tokenize input texts\n",
    "tokenized_texts = [tokenizer(text, padding=True, truncation=True, return_tensors='pt') for text in texts]\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(tokenized_texts['input_ids'], tokenized_texts['attention_mask'], torch.tensor(labels))\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids, attention_mask, label = batch\n",
    "        input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    average_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "\n",
    "    for batch in tqdm(val_loader, desc=f'Validation - Epoch {epoch + 1}/{num_epochs}'):\n",
    "        input_ids, attention_mask, label = batch\n",
    "        input_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_val_labels.extend(label.cpu().numpy())\n",
    "        all_val_preds.extend(preds)\n",
    "\n",
    "    val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "          f'Training Loss: {average_train_loss:.4f} - '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained('bert_model')\n",
    "tokenizer.save_pretrained('bert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ebee68-c4d1-4d0d-be48-5088664c5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "# Rest of the code remains the same as in the previous email example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7130e7b-24a8-4a83-86ee-d329a137c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\reena_savireddy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ae7a94b-4abb-4560-9e4f-cf6f007e6184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the content: jk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nPegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14140\\3662328107.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    787\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2022\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2024\u001b[1;33m         return cls._from_pretrained(\n\u001b[0m\u001b[0;32m   2025\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2026\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2254\u001b[0m         \u001b[1;31m# Instantiate the tokenizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2255\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2256\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2257\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2258\u001b[0m             raise OSError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\pegasus\\tokenization_pegasus_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, tokenizer_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mfrom_slow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_slow\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"<pad>\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meos_token\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"</s>\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"<unk>\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mfast_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslow_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslow_tokenizer_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;31m# We need to create and convert a slow tokenizer to build the backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[0mconverter_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSLOW_TO_FAST_CONVERTERS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokenizer_class_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconverter_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSpmConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"protobuf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nPegasusConverter requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "FileContent = input(\"enter the content:\")\n",
    "checkpoint = \"google/pegasus-large\"\n",
    "print(\"\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "# sentences = nltk.tokenize.sent_tokenize(FileContent)\n",
    "# print(max([len(tokenizer.tokenize(sentence)) for sentence in sentences]))\n",
    "\n",
    "\n",
    "# length = 0\n",
    "# chunk = \"\"\n",
    "# chunks = []\n",
    "# count = -1\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     count += 1\n",
    "#     combined_length = len(tokenizer.tokenize(sentence)) + length\n",
    "\n",
    "#     if combined_length <= 1000:\n",
    "#         chunk += sentence + \" \"\n",
    "#         length = combined_length\n",
    "\n",
    "#         if count == len(sentences) - 1:\n",
    "#             chunks.append(chunk.strip())\n",
    "\n",
    "#     else:\n",
    "#         chunks.append(chunk.strip())\n",
    "\n",
    "#         length = len(tokenizer.tokenize(sentence))\n",
    "#         chunk = sentence + \" \"\n",
    "\n",
    "# inputs = [tokenizer(chunk, return_tensors=\"pt\") for chunk in chunks]\n",
    "\n",
    "# for input in inputs:\n",
    "#   output = model.generate(**input)\n",
    "#   print(tokenizer.decode(*output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fc330d-414f-4b03-9e61-7f3443878c26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12580\\1061220304.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myfinance\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0myf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProphet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_plotly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_objs\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from datetime import date\n",
    "\n",
    "import yfinance as yf\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import plot_plotly\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "START = \"2015-01-01\"\n",
    "TODAY = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "st.title('Stock Forecast App')\n",
    "\n",
    "stocks = ('GOOG', 'AAPL', 'MSFT', 'GME')\n",
    "selected_stock = st.selectbox('Select dataset for prediction', stocks)\n",
    "\n",
    "n_years = st.slider('Years of prediction:', 1, 4)\n",
    "period = n_years * 365\n",
    "\n",
    "\n",
    "@st.cache\n",
    "def load_data(ticker):\n",
    "    data = yf.download(ticker, START, TODAY)\n",
    "    data.reset_index(inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "data_load_state = st.text('Loading data...')\n",
    "data = load_data(selected_stock)\n",
    "data_load_state.text('Loading data... done!')\n",
    "\n",
    "st.subheader('Raw data')\n",
    "st.write(data.tail())\n",
    "\n",
    "# Plot raw data\n",
    "def plot_raw_data():\n",
    "\tfig = go.Figure()\n",
    "\tfig.add_trace(go.Scatter(x=data['Date'], y=data['Open'], name=\"stock_open\"))\n",
    "\tfig.add_trace(go.Scatter(x=data['Date'], y=data['Close'], name=\"stock_close\"))\n",
    "\tfig.layout.update(title_text='Time Series data with Rangeslider', xaxis_rangeslider_visible=True)\n",
    "\tst.plotly_chart(fig)\n",
    "\n",
    "plot_raw_data()\n",
    "\n",
    "# Predict forecast with Prophet.\n",
    "df_train = data[['Date','Close']]\n",
    "df_train = df_train.rename(columns={\"Date\": \"ds\", \"Close\": \"y\"})\n",
    "\n",
    "m = Prophet()\n",
    "m.fit(df_train)\n",
    "future = m.make_future_dataframe(periods=period)\n",
    "forecast = m.predict(future)\n",
    "\n",
    "# Show and plot forecast\n",
    "st.subheader('Forecast data')\n",
    "st.write(forecast.tail())\n",
    "\n",
    "st.write(f'Forecast plot for {n_years} years')\n",
    "fig1 = plot_plotly(m, forecast)\n",
    "st.plotly_chart(fig1)\n",
    "\n",
    "st.write(\"Forecast components\")\n",
    "fig2 = m.plot_components(forecast)\n",
    "st.write(fig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16346938-4f6f-4355-9e12-cddcac94fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fbprophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc3090c-59c6-41ed-8ae7-b83837161426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pystan\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "defe0a40-2994-4108-8655-6f78c09ba85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter text Josh Seiden and Brian Donohue discuss the topic of outcome versus output on Inside Intercom. Josh Seiden is a product consultant and author who has just released a book called Outcomes Over Output. Brian is product management director and he's looking forward to the chat. The main premise of the book is that by defining outcomes precisely, it's possible to apply this idea of outcomes in our work. It's in contrast to a really broad and undefined definition of the word ”outcome”. Paul, one of the design managers at Intercom, was struggling to differentiate between customer outcomes and business impact. In Lean Startup, teams focus on what they can change in their behavior to make their customers more satisfied. They focus on the business impact instead of on the customer outcomes. They have a hypothesis and they test their hypothesis with an experiment. They don't have to be 100% certain, but they need to have a hunch. There is a difference between problem-focused and outcome-focused approaches to building and prioritizing projects. For example, a company is working on improving the inbox search feature in their product. They hope it will improve user retention and improve the business impact of the change. Product teams need to focus on the outcome of their work rather than on the business impact of their product. They need to be more aware of the customer experience and their relationship with their business. As a business owner, you have to build a theory of how the business works. The more you know about your business as your business goes on, the more you can build a business model. The business model is reflected in roadmaps and prioritizations. Josh's book is available on Amazon, in print, in ebook and in audiobook on Audible.com. Brian's advice for teams looking to change their way of working is to start small and to use retrospectives and improve your process as you try to implement this. Josh and Brian enjoyed their conversation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Josh Seiden and Brian Donohue discuss the topic of outcome versus output on Inside Intercom. Josh Seiden is a product consultant and author who has just released a book called Outcomes Over Output. Brian is product management director and he's looking forward to the chat. The main premise of the book is that by defining outcomes precisely, it's possible to apply this idea of outcomes in our work. It's in contrast to a really broad and undefined definition of the word ”outcome”. Paul, one of the design managers at Intercom, was struggling to differentiate between customer outcomes and business impact. In Lean Startup, teams focus on what they can change in their behavior to make their customers more satisfied. They focus on the business impact instead of on the customer outcomes. They have a hypothesis and they test their hypothesis with an experiment. They don't have to be 100% certain, but they need to have a hunch. There is a difference between problem-focused and outcome-focused approaches to building and prioritizing projects. For example, a company is working on improving the inbox search feature in their product. They hope it will improve user retention and improve the business impact of the change. Product teams need to focus on the outcome of their work rather than on the business impact of their product. They need to be more aware of the customer experience and their relationship with their business. As a business owner, you have to build a theory of how the business works. The more you know about your business as your business goes on, the more you can build a business model. The business model is reflected in roadmaps and prioritizations. Josh's book is available on Amazon, in print, in ebook and in audiobook on Audible.com. Brian's advice for teams looking to change their way of working is to start small and to use retrospectives and improve your process as you try to implement this. Josh and Brian enjoyed their conversation.\n",
      "\n",
      "Summary:\n",
      " Product teams need to focus on the outcome of their work rather than on the business impact of their product. There is a difference between problem-focused and outcome-focused approaches to building and prioritizing projects. As a business owner, you have to build a theory of how the business works. The more you know about your business as your business goes on, the more you can build a business model. The business model is reflected in roadmaps and prioritizations. Josh's book is available on Amazon, in print, in ebook and in audiobook on Audible.com. Brian's advice for teams looking to change their way of working is to start small and to use retrospectives and improve your process as you try to implement this. Josh Seiden is a product consultant and author who has just released a book called Outcomes Over Output. Brian is product management director and he's looking forward to the chat. The main premise of the book is that by defining outcomes precisely, it's possible to apply this idea of outcomes in our work.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "def bert_summarize(input_text):\n",
    "    # Load pre-trained BART model and tokenizer\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=500, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "input_text =input(\"enter text\")\n",
    "summary = bert_summarize(input_text)\n",
    "print(\"Original Text:\\n\", input_text)\n",
    "print(\"\\nSummary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001a44e-47c0-4133-b164-74f129775531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
